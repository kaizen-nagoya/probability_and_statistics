
# 1

BMI

Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)
Defense Advanced Research Projects Agency (DARPA)
Diversity in Faces (DiF) 
Equal Credit Opportunity Acts (ECOA)
Fair Housing Acts(FHA)
labeled faces in the wild (LFW)
maximum mean discrepancy (MMD)
price of fairness (POF)
reducing bias amplification (RBA)
Sentence Encoder Association Test (SEAT)
Science, Technology, Engineering, and Math (STEM)
Variational Auto Encoders(VAE)
Word Embedding Association Test (WEAT)



REFERENCES

[1] Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. 2019. Fair Regression: Quantitative Definitions and Reduction- Based Algorithms. In International Conference on Machine Learning. 120–129.

[2] Nazanin Alipourfard, Peter G Fennell, and Kristina Lerman. 2018. Can you Trust the Trend?: Discovering Simpson’s Paradoxes in Social Data. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 19–27.

[3]NazaninAlipourfard,PeterGFennell,andKristinaLerman.2018.UsingSimpsonâA ̆Z ́sParadoxtoDiscoverInteresting Patterns in Behavioral Data. In Twelfth International AAAI Conference on Web and Social Media.

[4] Alexander Amini, Ava Soleimany, Wilko Schwarting, Sangeeta Bhatia, and Daniela Rus. 2019. Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure. (2019).

[5]JuliaAngwin,JeffLarson,SuryaMattu,andLaurenKirchner.2016.MachineBias:thereâA ̆Z ́ssoftwareusedacrossthe countrytopredictfuturecriminals.AnditâA ̆Z ́sbiasedagainstblacks.ProPublica2016.(2016).

[6] A. Asuncion and D.J. Newman. 2007. UCI Machine Learning Repository. (2007). http://www.ics.uci.edu/$\sim$mlearn/

[7] Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. 2019. Scalable Fair Clustering. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, Long Beach, California, USA, 405–413. http://proceedings.mlr.press/v97/backurs19a.html

[8] Ricardo Baeza-Yates. 2018. Bias on the Web. Commun. ACM 61, 6 (May 2018), 54–61. https://doi.org/10.1145/3209581

[9] Samuel Barbosa, Dan Cosley, Amit Sharma, and Roberto M. Cesar-Jr. 2016. Averaging Gone Wrong: Using Time-Aware Analyses to Better Understand Behavior. (April 2016), 829–841.

[10] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. 2018. Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943 (2018).

[11] Emily M. Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics 6 (2018), 587–604. https://doi.org/10.1162/tacl_a_00041

[12] Misha Benjamin, Paul Gagnon, Negar Rostamzadeh, Chris Pal, Yoshua Bengio, and Alex Shee. [n. d.]. TOWARDS
STANDARDIZATION OF DATA LICENSES: THE MONTREAL DATA LICENSE. ([n. d.]).

[13] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and
Aaron Roth. 2017. A Convex Framework for Fair Regression. (2017). arXiv:cs.LG/1706.02409

[14] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. [n. d.]. Fairness in criminal justice risk
assessments: The state of the art. Sociological Methods & Research ([n. d.]), 0049124118782533.

[15] Peter J Bickel, Eugene A Hammel, and J William O’Connell. 1975. Sex bias in graduate admissions: Data from Berkeley.
Science 187, 4175 (1975), 398–404.

[16] RDP Binns. 2018. Fairness in machine learning: Lessons from political philosophy. Journal of Machine Learning
Research (2018).

[17] Colin R Blyth. 1972. On Simpson’s paradox and the sure-thing principle. J. Amer. Statist. Assoc. 67, 338 (1972),
364–366.

[18] Miranda Bogen and Aaron Rieke. 2018. Help wanted: an examination of hiring algorithms, equity. Technical Report.
and bias. Technical report, Upturn.

[19] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems. 4349–4357.

[20] Shikha Bordia and Samuel Bowman. 2019. Identifying and Reducing Gender Bias in Word-Level Language Models. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Student Research Workshop. 7–15.

[21] Avishek Bose and William Hamilton. 2019. Compositional Fairness Constraints for Graph Embeddings. In International
Conference on Machine Learning. 715–724.

[22] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the Origins of Bias in Word Embeddings. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, Long Beach, California, USA, 803–811. http://proceedings.mlr.press/v97/brunet19a.html

[23] Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research), Sorelle A. Friedler and Christo Wilson (Eds.), Vol. 81. PMLR, New York, NY, USA, 77–91. http://proceedings.mlr.press/v81/buolamwini18a.html

[24] Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery 21, 2 (2010), 277–292.

[25] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science 356, 6334 (2017), 183–186.

[26] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. 2017. Optimized Pre-Processing for Discrimination Prevention. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 3992–4001. http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention.pdf

[27] Manel Capdevila, Marta Ferrer, and Eulália Luque. 2005. La reincidencia en el delito en la justicia de menores. Centro de estudios jurídicos y formación especializada, Generalitat de Catalunya. Documento no publicado (2005).

[28] Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. 2018. How algorithmic confounding in recommenda- tion systems increases homogeneity and decreases utility. In Proceedings of the 12th ACM Conference on Recommender Systems. ACM, 224–232.

[29] Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, and Madeleine Udell. 2019. Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, 339–348.

[30] Xingyu Chen, Brandon Fain, Liang Lyu, and Kamesh Munagala. 2019. Proportionally Fair Clustering. In International Conference on Machine Learning. 1032–1041.

[31] Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5, 2 (2017), 153–163.

[32] Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research), Sorelle A. Friedler and Christo Wilson (Eds.), Vol. 81. PMLR, New York, NY, USA, 134–148. http://proceedings.mlr.press/v81/ chouldechova18a.html

[33] Alexandra Chouldechova and Aaron Roth. 2018. The frontiers of fairness in machine learning. arXiv preprint arXiv:1810.08810 (2018).

[34] John S. Chuang, Olivier Rivoire, and Stanislas Leibler. 2009. Simpson’s Paradox in a Synthetic Microbial System. Science 323, 5911 (2009), 272–275. https://doi.org/10.1126/science.1166739 arXiv:https://science.sciencemag.org/content/323/5911/272.full.pdf

[35] Lee Cohen, Zachary C. Lipton, and Yishay Mansour. 2019. Efficient candidate screening under multiple tests and implications for fairness. (2019). arXiv:cs.LG/1905.11361

[36] United States. Equal Employment Opportunity Commission. [n. d.]. EEOC compliance manual. [Washington, D.C.] : U.S. Equal Employment Opportunity Commission, [1992].

[37] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 797–806.

[38] Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. 2019. Flexibly Fair Representation Learning by Disentanglement. In International Conference on Machine Learning. 1436–1445.

[39] Brian d’Alessandro, Cathy O’Neil, and Tom LaGatta. 2017. Conscientious classification: A data scientist’s guide to discrimination-aware classification. Big data 5, 2 (2017), 120–134.

[40] Shai Danziger, Jonathan Levav, and Liora Avnaim-Pesso. 2011. Extraneous factors in judicial decisions. Proceedings of the National Academy of Sciences 108, 17 (2011), 6889–6892.

[41] Julia Dressel and Hany Farid. 2018. The accuracy, fairness, and limits of predicting recidivism. Science Advances 4, 1 (2018). https://doi.org/10.1126/sciadv.aao5580 arXiv:https://advances.sciencemag.org/content/4/1/eaao5580.full.pdf

[42] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. (2017). http://archive.ics.uci.edu/ml

[43] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness Through Awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS ’12). ACM, New York, NY,
USA, 214–226. https://doi.org/10.1145/2090236.2090255

[44] Golnoosh Farnadi, Behrouz Babaki, and Lise Getoor. 2018. Fairness in Relational Domains. In Proceedings of the
2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES ’18). ACM, New York, NY, USA, 108–114. https:
//doi.org/10.1145/3278721.3278733

[45] Joel Escudé Font and Marta R Costa-jussà. 2019. Equalizing Gender Biases in Neural Machine Translation with Word
Embeddings Techniques. arXiv preprint arXiv:1901.03116 (2019).

[46] Batya Friedman and Helen Nissenbaum. 1996. Bias in Computer Systems. ACM Trans. Inf. Syst. 14, 3 (July 1996),
330–347. https://doi.org/10.1145/230538.230561

[47] Anna Fry, Thomas J Littlejohns, Cathie Sudlow, Nicola Doherty, Ligia Adamska, Tim Sprosen, Rory Collins, and
Naomi E Allen. 2017. Comparison of Sociodemographic and Health-Related Characteristics of UK Biobank Participants With Those of the General Population. American Journal of Epidemiology 186, 9 (06 2017), 1026–1034. https: //doi.org/10.1093/aje/kwx246 arXiv:http://oup.prod.sis.lan/aje/article-pdf/186/9/1026/24330720/kwx246.pdf

[48] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. [n. d.]. Datasheets for Datasets. ([n. d.]).

[49] Naman Goel, Mohammad Yaghini, and Boi Faltings. 2018. Non-discriminatory machine learning through convex fairness criteria. In Thirty-Second AAAI Conference on Artificial Intelligence.

[50] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. arXiv preprint arXiv:1903.03862 (2019).

[51] Sandra González-Bailón, Ning Wang, Alejandro Rivero, Javier Borge-Holthoefer, and Yamir Moreno. 2014. Assessing the bias in samples of large online networks. Social Networks 38 (2014), 16–27.

[52] Susan T Gooden. 2015. Race and social equity: A nervous area of government. Routledge.

[53] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. 2016. The case for process fairness in learning: Feature selection for fair decision making. In NIPS Symposium on Machine Learning and the Law, Vol. 1. 2.

[54] S. Hajian and J. Domingo-Ferrer. 2013. A Methodology for Direct and Indirect Discrimination Prevention in Data
Mining. IEEE Transactions on Knowledge and Data Engineering 25, 7 (July 2013), 1445–1459. https://doi.org/10.
1109/TKDE.2012.72

[55] Moritz Hardt, Eric Price, Nati Srebro, et al. 2016. Equality of opportunity in supervised learning. In Advances in neural
information processing systems. 3315–3323.

[56] Eszter Hargittai. 2007. Whose Space? Differences among Users and Non-Users of Social Network Sites. Journal
of Computer-Mediated Communication 13, 1 (10 2007), 276–297. https://doi.org/10.1111/j.1083-6101.2007.00396.x
arXiv:http://oup.prod.sis.lan/jcmc/article-pdf/13/1/276/22317170/jjcmcom0276.pdf

[57] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The dataset nutrition label:
A framework to drive higher data quality standards. arXiv preprint arXiv:1805.03677 (2018).

[58] Ayanna Howard and Jason Borenstein. 2018. The ugly truth about ourselves and our robot creations: the problem of bias
and social inequity. Science and engineering ethics 24, 5 (2018), 1521–1536.

[59] Lingxiao Huang and Nisheeth Vishnoi. 2019. Stable and Fair Classification. In International Conference on Machine
Learning. 2879–2890.

[60] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un) fairness: Lessons for Machine Learning. In
Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, 49–58.

[61] L. Introna and H. Nissenbaum. 2000. Defining the Web: the politics of search engines. Computer 33, 1 (Jan 2000),
54–62. https://doi.org/10.1109/2.816269

[62] Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan. 2018. Unsupervised Adversarial Invariance.
(2018). arXiv:cs.LG/1809.10083

[63] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for classification without discrimination.
Knowledge and Information Systems 33, 1 (01 Oct 2012), 1–33. https://doi.org/10.1007/s10115-011-0463-8

[64] Faisal Kamiran and Indre ̇ Žliobaite ̇. 2013. Explainable and Non-explainable Discrimination in Classification. Springer
Berlin Heidelberg, Berlin, Heidelberg, 155–170. https://doi.org/10.1007/978-3-642-30487-3_8

[65] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware classifier with prejudice remover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 35–50.

[66] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness. In International Conference on Machine Learning. 2569–2577.

[67] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2019. An empirical study of rich subgroup fairness for machine learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, 100–109.

[68] Rogier Kievit, Willem Eduard Frankenhuis, Lourens Waldorp, and Denny Borsboom. 2013. Simpson’s paradox in psychological science: a practical guide. Frontiers in psychology 4 (2013), 513.

[69] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf. 2017. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems. 656–666.

[70] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807 (2016).

[71] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, Vol. 5. 79–86.

[72] Emmanouil Krasanakis, Eleftherios Spyromitros-Xioufis, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification. In Proceedings of the 2018 World
Wide Web Conference (WWW ’18). International World Wide Web Conferences Steering Committee, Republic and
Canton of Geneva, Switzerland, 853–862. https://doi.org/10.1145/3178876.3186133

[73] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In Advances in Neural
Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (Eds.). Curran Associates, Inc., 4066–4076. http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf

[74] Anja Lambrecht and Catherine E Tucker. 2018. Algorithmic bias? An empirical study into apparent gender-based discrimination in the display of STEM career ads. An Empirical Study into Apparent Gender-Based Discrimination in
the Display of STEM Career Ads (March 9, 2018) (2018).

[75] J Larson, S Mattu, L Kirchner, and J Angwin. 2016. Compas analysis. GitHub, available at: https://github.
com/propublica/compas-analysis[Google Scholar] (2016).

[76] Blake Lemoine, Brian Zhang, and M Mitchell. 2018. Mitigating Unwanted Biases with Adversarial Learning. (2018).

[77]KristinaLerman.2018.Computationalsocialscientistbeware:SimpsonâA ̆Z ́sparadoxinbehavioraldata.Journalof
Computational Social Science 1, 1 (2018), 49–58.

[78] Kristina Lerman and Tad Hogg. 2014. Leveraging position bias to improve peer recommendation. PloS one 9, 6 (2014),
e98914. [79]ZacharyCLipton,AlexandraChouldechova,andJulianMcAuley.2017.DoesmitigatingMLâA ̆Z ́sdisparateimpact
require disparate treatment? stat 1050 (2017), 19.

[80] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. 2018. Delayed Impact of Fair Machine
Learning. In Proceedings of the 35th International Conference on Machine Learning.

[81] Joshua R Loftus, Chris Russell, Matt J Kusner, and Ricardo Silva. 2018. Causal reasoning for algorithmic fairness.
arXiv preprint arXiv:1805.05859 (2018).

[82] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. 2016. THE VARIATIONAL FAIR
AUTOENCODER. stat 1050 (2016), 4.

[83] Arjun K. Manrai, Birgit H. Funke, Heidi L. Rehm, Morten S. Olesen, Bradley A. Maron, Peter Szolovits, David M.
Margulies, Joseph Loscalzo, and Isaac S. Kohane. 2016. Genetic Misdiagnoses and the Potential for Health Dis- parities. New England Journal of Medicine 375, 7 (2016), 655–665. https://doi.org/10.1056/NEJMsa1507092 arXiv:https://doi.org/10.1056/NEJMsa1507092 PMID: 27532831.

[84] Chandler May, Alex Wang, Shikha Bordia, Samuel R Bowman, and Rachel Rudinger. 2019. On measuring social biases in sentence encoders. arXiv preprint arXiv:1903.10561 (2019).

[85] Ninareh Mehrabi, Fred Morstatter, Nanyun Peng, and Aram Galstyan. 2019. Debiasing Community Detection: The Importance of Lowly-Connected Nodes. arXiv preprint arXiv:1903.08136 (2019).

[86] Aditya Krishna Menon and Robert C Williamson. 2018. The cost of fairness in binary classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research), Sorelle A. Friedler and Christo Wilson (Eds.), Vol. 81. PMLR, New York, NY, USA, 107–118. http://proceedings.mlr. press/v81/menon18a.html

[87] Michele Merler, Nalini Ratha, Rogerio S Feris, and John R Smith. 2019. Diversity in Faces. arXiv preprint arXiv:1901.10436 (2019).

[88] Hannah Jean Miller, Jacob Thebault-Spieker, Shuo Chang, Isaac Johnson, Loren Terveen, and Brent Hecht. 2016. âA ̆IJBlissfullyHappyâA ̆ ̇IorâA ̆IJReadytoFightâA ̆ ̇I:VaryingInterpretationsofEmoji.InTenthInternationalAAAI Conference on Web and Social Media.

[89] I Minchev, G Matijevic, DW Hogg, G Guiglion, M Steinmetz, F Anders, C Chiappini, M Martig, A Queiroz, and C Scannapieco. 2019. Yule-Simpson’s paradox in Galactic Archaeology. arXiv preprint arXiv:1902.01421 (2019).

[90] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,
Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19). ACM, New York, NY, USA, 220–229. https://doi.org/10. 1145/3287560.3287596

[91] Fred Morstatter, Jürgen Pfeffer, Huan Liu, and Kathleen M Carley. 2013. Is the sample good enough? Comparing data from twitter’s streaming API with Twitter’s firehose. In 7th International AAAI Conference on Weblogs and Social Media, ICWSM 2013. AAAI press.

[92] Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg. 2018. Invariant Representations without Adversarial Training. In Advances in Neural Information Processing Systems. 9084–9093.

[93] Amitabha Mukerjee, Rita Biswas, Kalyanmoy Deb, and Amrit P Mathur. 2002. Multi–objective evolutionary algorithms for the risk–return trade–off in bank loan management. International Transactions in operational research 9, 5 (2002), 583–597.

[94] Razieh Nabi, Daniel Malinsky, and Ilya Shpitser. 2018. Learning Optimal Fair Policies. arXiv preprint arXiv:1809.02244 (2018).

[95] Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In Thirty-Second AAAI Conference on Artificial Intelligence.

[96] Azadeh Nematzadeh, Giovanni Luca Ciampaglia, Filippo Menczer, and Alessandro Flammini. 2017. How algorithmic popularity bias hinders or promotes quality. arXiv preprint arXiv:1707.00574 (2017).

[97] Dong-Phuong Nguyen, Rilana Gravel, Rudolf Berend Trieschnigg, and Theo Meder. 2013. "How old do you think I am?": A study of language and age in Twitter. In Proceedings of the Seventh International AAAI Conference on Weblogs andSocialMedia,ICWSM2013.AAAIPress,439–448. eemcs-eprint-23604.

[98] Anne O’Keeffe and Michael McCarthy. 2010. The Routledge handbook of corpus linguistics. Routledge.

[99] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2016. Social data: Biases, methodological
pitfalls, and ethical boundaries. (2016).

[100] Cathy O’Neil. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy.
Crown Publishing Group, New York, NY, USA.

[101] Osonde A Osoba and William Welser IV. 2017. An intelligence in our image: The risks of bias and errors in artificial
intelligence. Rand Corporation.

[102] Edmund S Phelps. 1972. The statistical theory of racism and sexism. The american economic review 62, 4 (1972),
659–661.

[103] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. 2017. On Fairness and Cali-
bration. In Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 5680–5689. http://papers.nips.cc/paper/ 7151- on- fairness- and- calibration.pdf

[104] Marcelo OR Prates, Pedro H Avelar, and Luís C Lamb. 2018. Assessing gender bias in machine translation: a case study with Google Translate. Neural Computing and Applications (2018), 1–19.

[105] Bilal Qureshi, Faisal Kamiran, Asim Karim, and Salvatore Ruggieri. 2016. Causal discrimination discovery through propensity score analysis. arXiv preprint arXiv:1608.03735 (2016).

[106] Inioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products.

[107] M Redmond. 2011. Communities and crime unnormalized data set. UCI Machine Learning Repository. In website: http://www. ics. uci. edu/mlearn/MLRepository. html (2011).

[108] Lauren A Rivera. 2012. Hiring as cultural matching: The case of elite professional service firms. American sociological review 77, 6 (2012), 999–1022.

[109] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender Bias in Coreference Resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). Association for Computational Linguistics, New Orleans, Louisiana, 8–14. https://doi.org/10.18653/v1/N18-2002

[110] Pedro Saleiro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. 2018. Aequitas: A Bias and Fairness Audit Toolkit. arXiv preprint arXiv:1811.05577 (2018).

[111] Samira Samadi, Uthaipon Tantipongpipat, Jamie Morgenstern, Mohit Singh, and Santosh Vempala. 2018. The Price of Fair PCA: One Extra Dimension. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems (NIPS’18). Curran Associates Inc., USA, 10999–11010. http://dl.acm.org/citation.cfm?id=3327546.3327755

[112] Nripsuta Ani Saxena. 2019. Perceptions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (AIES ’19). ACM, New York, NY, USA, 537–538. https://doi.org/10.1145/3306618.3314314 Mehrabi et al.

[113] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C Parkes, and Yang Liu. 2019. How Do Fairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. ACM, 99–106.

[114] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recom- mendations as Treatments: Debiasing Learning and Evaluation. In International Conference on Machine Learning. 1670–1679.

[115] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, 59–68.

[116] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. 2017. No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. stat 1050 (2017), 22.

[117] Richard Shaw and Manuel Corpas. [n. d.]. Further bias in personal genomics? ([n. d.]).

[118] Harini Suresh and John V Guttag. 2019. A Framework for Understanding Unintended Consequences of Machine
Learning. arXiv preprint arXiv:1901.10002 (2019).

[119] Songül Tolan, Marius Miron, Emilia Gómez, and Carlos Castillo. 2019. Why Machine Learning May Lead to
Unfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia. (2019).

[120] Zeynep Tufekci. 2014. Big questions for social media big data: Representativeness, validity and other methodological
pitfalls. In Eighth International AAAI Conference on Weblogs and Social Media.

[121] Berk Ustun, Yang Liu, and David Parkes. 2019. Fairness without Harm: Decoupled Classifiers with Preference
Guarantees. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR, Long Beach, California, USA, 6373–6382. http://proceedings.mlr.press/v97/ustun19a.html

[122] Eva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 3003–3008.

[123] Sahil Verma and Julia Rubin. 2018. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare). IEEE, 1–7.

[124] Selwyn Vickers, Mona Fouad, and Moon S Chen Jr. 2014. Enhancing Minority Participation in Clinical Trials (EMPaCT): laying the groundwork for improving minority clinical trial accrual. Cancer 120 (2014), vi–vii.

[125] Ting Wang and Dashun Wang. 2014. Why Amazon’s ratings might mislead you: The story of herding effects. Big data 2, 4 (2014), 196–204.

[126] Christo Wilson, Bryce Boe, Alessandra Sala, Krishna PN Puttaswamy, and Ben Y Zhao. 2009. User interactions in social networks and their implications. In Proceedings of the 4th ACM European conference on Computer systems. Acm, 205–218.

[127] Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. 2017. Learning non-discriminatory predictors. arXiv preprint arXiv:1702.06081 (2017).

[128] Yongkai Wu, Lu Zhang, and Xintao Wu. 2018. Fairness-aware Classification: Criterion, Convexity, and Bounds. (2018). arXiv:cs.LG/1809.04737

[129] Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. 2018. Fairgan: Fairness-aware generative adversarial networks. In 2018 IEEE International Conference on Big Data (Big Data). IEEE, 570–575.

[130] Irene Y Chen, Peter Szolovits, and Marzyeh Ghassemi. 2019. Can AI Help Reduce Disparities in General Medical and Mental Health Care? AMA journal of ethics 21 (02 2019), E167–179. https://doi.org/10.1001/amajethics.2019.167

[131] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. 2015. Fairness constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259 (2015).

[132] Lu Zhang and Xintao Wu. 2017. Anti-discrimination learning: a causal modeling-based framework. International Journal of Data Science and Analytics 4, 1 (01 Aug 2017), 1–16. https://doi.org/10.1007/s41060-017-0058-x

[133] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. On Discrimination Discovery Using Causal Networks. In Social, Cultural, and Behavioral Modeling, Kevin S. Xu, David Reitter, Dongwon Lee, and Nathaniel Osgood (Eds.). Springer International Publishing, Cham, 83–93.

[134] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. Situation Testing-based Discrimination Discovery: A Causal Inference Approach. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI’16). AAAI Press, 2718–2724. http://dl.acm.org/citation.cfm?id=3060832.3061001

[135] Lu Zhang, Yongkai Wu, and Xintao Wu. 2017. Achieving non-discrimination in data release. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1335–1344.

[136] Lu Zhang, Yongkai Wu, and Xintao Wu. 2017. A Causal Framework for Discovering and Removing Direct and Indirect Discrimination. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17. 3929–3935. https://doi.org/10.24963/ijcai.2017/549

[137] L. Zhang, Y. Wu, and X. Wu. 2018. Causal Modeling-Based Discrimination Discovery and Removal: Criteria, Bounds, and Algorithms. IEEE Transactions on Knowledge and Data Engineering (2018), 1–1. https://doi.org/10.1109/TKDE. 2018.2872988

[138] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender Bias in Contextualized Word Embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 629–634.

[139] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.

[140] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods. (2018). arXiv:cs.CL/1804.06876

[141] Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-Wei Chang. 2018. Learning Gender-Neutral Word Embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 4847–4853.

[142] James Zou and Londa Schiebinger. 2018. AI can be sexist and racist it’s time to make it fair. (2018).

# 2 
GPU

Batch Normalization (BN)
NVIDIA Collective Communication Library (NCCL)3
stochastic gradient descent(SGD)


References

[1] J. Bagga, H. Morsy, and Z. Yao. Opening
designs for 6-pack and Wedge 100. https://code.facebook.com/posts/203733993317833/opening-designs-for-6-pack-and-wedge-100, 2016.

[2] M. Barnett, L. Shuler, R. van De Geijn, S. Gupta, D. G.
Payne, and J. Watts. Interprocessor collective communication
library (intercom). In Scalable High-Performance Computing
Conference, 1994.

[3] L. Bottou. Curiously fast convergence of some stochastic
gradient descent algorithms. Unpublished open problem offered
to the attendance of the SLDS 2009 conference, 2009.

[4] L. Bottou, F. E. Curtis, and J. Nocedal. Opt. methods for
large-scale machine learning. arXiv:1606.04838, 2016.

[5] J. Chen, X. Pan, R. Monga, S. Bengio, and R. Jozefowicz.
Revisiting Distributed Synchronous SGD.
arXiv:1604.00981, 2016.

[6] K. Chen and Q. Huo. Scalable training of deep learning machines
by incremental block training with intra-block parallel
optimization and blockwise model-update filtering. In
ICASSP, 2016.

[7] R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. Natural language processing
(almost) from scratch. JMLR, 2011.

[8] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation
feature for generic visual recognition. In ICML, 2014.

[9] R. Girshick. Fast R-CNN. In ICCV, 2015.

[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature
hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.

[11] W. Gropp, E. Lusk, and A. Skjellum. Using MPI: Portable
Parallel Programming with the Message-Passing Interface.
MIT Press, Cambridge, MA, 1999.

[12] S. Gross and M. Wilber. Training and investigating Residual
Nets. https://github.com/facebook/fb.resnet.torch, 2016.

[13] M. G¨urb¨uzbalaban, A. Ozdaglar, and P. Parrilo. Why
random reshuffling beats stochastic gradient descent.
arXiv:1510.08560, 2015.

[14] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask RCNN.
arXiv:1703.06870, 2017.

[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectifiers: Surpassing human-level performance on imagenet
classification. In ICCV, 2015.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, 2016.

[17] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath,
et al. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE
Signal Processing Magazine, 2012.

[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and
Y. Bengio. Quantized neural networks: Training neural
networks with low precision weights and activations.
arXiv:1510.08560, 2016.

[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015.

[20] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and
P. T. P. Tang. On large-batch training for deep learning: Generalization
gap and sharp minima. ICLR, 2017.

[21] A. Krizhevsky. One weird trick for parallelizing convolutional
neural networks. arXiv:1404.5997, 2014.

[22] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification
with deep convolutional neural nets. In NIPS, 2012.

[23] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation
applied to handwritten zip code recognition. Neural computation,
1989.

[24] K. Lee. Introducing Big Basin: Our next-generation
AI hardware. https://code.facebook.com/posts/
1835166200089399/introducing-big-basin, 2017.

[25] M. Li. Scaling Distributed Machine Learning with System
and Algorithm Co-design. PhD thesis, Carnegie Mellon University,
2017.

[26] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In CVPR, 2017.

[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Common
objects in context. In ECCV. 2014.

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015.

[29] Y. Nesterov. Introductory lectures on convex optimization: A
basic course. Springer, 2004.

[30] R. Rabenseifner. Optimization of collective reduction operations.
In ICCS. Springer, 2004.

[31] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards
real-time object detection with region proposal networks.
In NIPS, 2015.

[32] H. Robbins and S. Monro. A stochastic approximation
method. The annals of mathematical statistics, 1951.

[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015.

[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, localization
and detection using convolutional networks. In ICLR, 2014.

[35] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.

[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.

[37] R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of
collective comm. operations in MPICH. IJHPCA, 2005.

[38] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,
W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al. Google’s neural machine translation system: Bridging
the gap between human and machine translation.
arXiv:1609.08144, 2016.

[39] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. In CVPR,
2017.

[40] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke,
D. Yu, and G. Zweig. The Microsoft 2016 Conversational
Speech Recognition System. arXiv:1609.03528, 2016.

[41] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional neural networks. In ECCV, 2014.

# 3

Constant Uncertainty Dropout (CUDO).
Continuous Ranked Probability Score (CRPS)
cross-validation (CV)
Monte Carlo Dropout (MCDO)
Monte Carlo Batch Normalization (MCBN)
Multiplicative Normalizing Flows for variational Bayesian networks (MNF)
Log likelihood (PLL)
variational inference (VI)
Probabilistic backpropagation (PBP)
Kullback-Leibler (KL)

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-
Lobato, J. M., and Turner, R. E. Deep Gaussian Processes
for Regression using Approximate Expectation
Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Urtasun,
R. Monocular 3d object detection for autonomous
driving. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2147–2156,
2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Precision
histology: how deep learning is poised to revitalize
histomorphology for personalized cancer care. npj Precision
Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M.,
Blau, H. M., and Thrun, S. Dermatologist-level classification
of skin cancer with deep neural networks. Nature,
Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, University
of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Approximation
: Representing Model Uncertainty in Deep
Learning. ICML, 48:1–10, 2015.

Ghahramani, Z. Delve Datasets. University of Toronto,
1996. URL http://www.cs.toronto.edu/
{˜}delve/data/kin/desc.html.

Ghahramani, Z. Probabilistic machine learning and artificial
intelligence. Nature, 521(7553):452–459, May
2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring
Rules, Prediction, and Estimation. Journal of the American
Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining
and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Networks.
NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic
backpropagation for scalable learning of bayesian neural
networks. In International Conference on Machine
Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural networks
simple by minimizing the description length of
the weights. In Proceedings of the sixth annual conference
on Computational learning theory, pp. 5–13. ACM,
1993.

Ioffe, S. Batch renormalization: Towards reducing minibatch
dependence in batch-normalized models. CoRR,
abs/1702.03275, 2017. URL http://arxiv.org/
abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating
Deep Network Training by Reducing Internal Covariate
Shift. Arxiv, 2015. URL http://arxiv.
org/abs/1502.03167.

Karpathy, A. Convnetjs demo: toy 1d regression, 2015.
URL http://cs.stanford.edu/people/karpathy/convnetjs/demo/regression.html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian
SegNet: Model Uncertainty in Deep Convolutional
Encoder-Decoder Architectures for Scene Understanding.
CoRR, abs/1511.0, 2015. URL http://arxiv.org/abs/1511.02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational
Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of
features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste,
A., and Courville, A. Bayesian hypernetworks. arXiv
preprint arXiv:1710.04759, 2017.

Lehmann, E. L. Elements of Large-Sample Theory.
Springer Verlag, New York, 1999. ISBN 0387985956.
Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural
Networks with Alpha-divergences. arXiv, 2017.

Louizos, C. and Welling, M. Multiplicative normalizing
flows for variational Bayesian neural networks.
In Precup, D. and Teh, Y. W. (eds.), Proceedings of
the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning
Research, pp. 2218–2227, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/louizos17a.html.

MacKay, D. J. A practical bayesian framework for backpropagation
networks. Neural computation, 4(3):448–
472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD
thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume
118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scoring
rule. Experimental Economics, 1(1):43–62, 1998.
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Shen, L. End-to-end training for whole image breast cancer
diagnosis using an all convolutional design. arXiv
preprint arXiv:1708.09427, 2017.
University of California, I. UC Irvine Machine Learning
Repository, 2017. URL https://archive.ics.uci.edu/ml/index.html.

Wang, S. I. and Manning, C. D. Fast dropout training.
Proceedings of the 30th International Conference
on Machine Learning, 28:118–126, 2013.
URL http://machinelearning.wustl.edu/mlpapers/papers/wang13a.
